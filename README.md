# Automated Legacy Code Comments Evaluation

This repository contains the implementation and evaluation pipeline for the paper "Automated Legacy Code Comments: Evaluating Large Language Models vs. Traditional Methods". The pipeline compares comments generated by Code Llama against traditional methods (Roslyn Analyzer) and original developer comments.

## Prerequisites

- Python 3.8+
- Ollama (for Code Llama)
- NVIDIA GPU recommended but not required
- Dependencies listed in requirements.txt

## Installation

1. Clone the repository:
```bash
git clone [repository-url]
cd automated-legacy-code-comments
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install Ollama and the Code Llama model:
```bash
# Install Ollama from https://ollama.ai/
ollama pull codellama
```

## Repository Structure

```
reproducible-code-comments/
├── README.md
├── requirements.txt
├── config.json                 # Configuration file
├── run.py                     # Main pipeline script
├── src/
│   ├── generators/
│   │   └── llm_generator.py   # Code Llama integration
│   ├── matchers/
│   │   └── comment_matcher.py # Comment alignment
│   └── evaluators/            # Evaluation metrics
│       ├── semantic_evaluator.py
│       ├── rouge_evaluator.py
│       ├── jaccard_evaluator.py
│       └── readability_evaluator.py
├── data/
│   ├── raw/                   # Input data
│   ├── interim/               # Intermediate results
│   └── processed/             # Final results
└── logs/                      # Pipeline logs
```

## Data Preparation

Place your input files in the `data/raw/` directory:
1. `500_methods.csv` - Methods for comment generation
2. `500_methods_and_summaries.csv` - Original developer comments
3. `500_comment_generated_nonllm.csv` - Pre-generated Roslyn comments

## Configuration

The `config.json` file contains all configurable parameters:
- Input/output file paths
- LLM model settings
- Evaluation metrics configuration

## Running the Pipeline

1. Ensure all input files are in place
2. Run the pipeline:
```bash
python run.py
```

The pipeline will:
1. Generate comments using Code Llama
2. Match comments from all sources
3. Run evaluations (semantic similarity, ROUGE, Jaccard, readability)
4. Save results in `data/processed/`

## Output Files

The pipeline generates several output files in `data/processed/`:
- `semantic_results.csv` - Semantic similarity evaluation
- `rouge_results.csv` - ROUGE scores
- `jaccard_results.csv` - Jaccard similarity results
- `readability_results.csv` - Readability metrics
- `evaluation_stats.json` - Aggregated statistics

## Evaluation Metrics

1. **Semantic Similarity**
   - Uses CodeBERT embeddings
   - Cosine similarity comparison

2. **ROUGE Scores**
   - ROUGE-1 and ROUGE-2 metrics
   - Evaluates content overlap

3. **Jaccard Similarity**
   - Measures lexical overlap
   - Token-based comparison

4. **Readability**
   - Flesch-Kincaid Grade Level
   - SMOG Index

## Logging

The pipeline logs all operations to:
- Console output
- Log files in the `logs/` directory

## Requirements

Key dependencies (see requirements.txt for complete list):
- transformers
- torch
- nltk
- pandas
- numpy
- requests
- tqdm
- matplotlib
- seaborn

